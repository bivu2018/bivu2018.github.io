<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Brave New Ideas in Video Understanding</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/sl-slide.css">

    <script src="js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    <script src="data/id2path.json" charset="utf-8"></script>
    

    <script src="js/animate.js"></script>

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="images/ico/favicon.ico">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="images/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="images/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="images/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="images/ico/apple-touch-icon-57-precomposed.png">
</head>

<body>

    <!--Header-->
    <header class="navbar navbar-fixed-top">
        <div class="navbar-inner">
            <div class="container">
                <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </a>
                <a id="logo" class="pull-left" href="index.html"></a>
                <div class="nav-collapse collapse pull-right">
                    <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="#about">About</a></li>
                        <li><a href="#program">Program &#38; Accepted Papers</a></li>
                        <li><a href="#dates">Important Dates</a></li>
                        <li><a href="#submission">Submission</a></li> 
                        <li><a href="#registration">Registration & Venue</a></li>
                        <li><a href="#organizers">Organizers</a></li>
                    </ul>        
                </div><!--/.nav-collapse -->
            </div>
        </div>
    </header>
    <!-- /header -->

    <!--Slider-->
    <section id="slide-show">
     <div id="slider" class="sl-slider-wrapper">

        <!--Slider Items-->    
        <div class="sl-slider">
            <!--Slider Item1-->
            <div class="sl-slide item1" data-orientation="horizontal" data-slice1-rotation="-25" data-slice2-rotation="-25" data-slice1-scale="2" data-slice2-scale="2">
                <div class="sl-slide-inner">
                    <div class="container">
                        <canvas class="pull-right" height="420px" width="800px" id="aniCanvas"></canvas>
                        <h2>Wanted:</h2>
                        <h3 class="gap">Brave New Ideas for Video Understanding</h3>
                    </div>
                </div>
            </div>
            <!--/Slider Item1-->

    </div>
    <!--/Slider Items-->


</div>
<!-- /slider-wrapper -->           
</section>
<!--/Slider-->

<section id="about" >
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>CVPR'18 Workshop: <br/>Brave New Ideas for Video Understanding</h1>
                <h3>Date: June 18th, 2018, 09.00-17.15</h3>
                <p class="lead">Together with the <a href="http://cvpr2018.thecvf.com/">Computer Vision 
                and Pattern Recognition (CVPR) 2018</a>.
                </p> 
                
<!--                 This is the evolution of <a href="http://bravenewmotion.github.io/eccv16/index.html">previous CVPR17 and ECCV16 successful workshops -->
                
                
                <h3>Description of the workshop and its relevance</h3>
                <p class="lead">
                In the late years Deep Learning has been a great force of change on most computer vision tasks. In video analysis problems, however, such as action recognition and detection, motion analysis and tracking, shallow architectures remain surprisingly competitive. What is the reason for this conundrum?  Larger datasets are part of the solution. The recently proposed Sports1M and Kinetics helped recently in the realistic training of large motion networks. Still, the breakthrough has not yet arrived.
                </p>

                <p class="lead">
                Assuming that the recently proposed video datasets are large enough for training deep networks for video, another likely culprit for the standstill in video analysis is the capacity of the existing deep models. More specifically, the existing deep networks for video analysis might not be sophisticated enough to address the complexity of motion information. This makes sense, as videos introduce an exponential complexity as compared to static images. Unfortunately, state-of-the-art motion representation models are extensions of existing image representations rather than motion dedicated ones. Brave, new and motion-specific representations are likely to be needed for a breakthrough in video analysis.
                </p>

                <p class="lead">
                The goal of this workshop is to bring together researchers from the broad area of video analysis to discuss problem statements, evaluation metrics, and benchmarks that will spur disruptive progress in the field of video understanding. The workshop will include a series of invited talks by leading researchers in this area as well as oral and poster presentations of accepted papers.
                </p>


                <h3>Calling papers for brave new ideas</h3>
                <p class="lead">

                Submissions will be in the form of short non- anonymous papers and will consist of a maximum of 4 pages (excluding references). Submissions must represent new work, i.e., work that has not been previously published or accepted for publication. However, papers that expand previous related work by the authors and papers that have appeared on non peer-reviewed websites (such as arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) are acceptable. Accepted papers will be presented as posters or contributed talks. Authors of accepted papers will be asked to post their submissions on arXiv. The workshop website will provide links to the accepted papers on arXiv. Accepted papers will be considered non- archival, and may be submitted elsewhere (modified or not).
                </p>

                <h3>Expert speakers</h3>
                <p class="lead">
                To kickstart the discussion we will have several influential speakers.
                </p>

                <h3>Topics</h3>
                <p class="lead">
                The workshop focuses on video representations related, but not limited, to the following topics: </p>
               
               <p class="lead">
                - Influence of motion in object recognition, object affordance, scene understanding.<br/>
                - Object and optical flow<br/>
                - Motion prediction, causal reasoning and forecasting<br/>
                - Event and action recognition <br/>
                - Spatio-temporal action localization<br/>
                - Modeling human motion in videos and video streams<br/>
                - Motion segmentation and saliency<br/>
                - Tracking of objects in space and time<br/>
                - Unsupervised action, actom discovery using ego motion<br/>
                - Applications of motion understanding and video dynamics in sports, healthcare, autonomous driving, driver assistance and robotics<br/>
                </p>

                <p class="lead">
                - Influence of motion in object recognition, object affordance, scene understanding.<br/>
                - Object and optical flow<br/>
                - Motion prediction, causal reasoning and forecasting<br/>
                - Event and action recognition <br/>
                - Spatio-temporal action localization<br/>
                - Modeling human motion in videos and video streams<br/>
                - Motion segmentation and saliency<br/>
                - Tracking of objects in space and time<br/>
                - Unsupervised action, actom discovery using ego motion<br/>
                - Applications of motion understanding and video dynamics in sports, healthcare, autonomous driving, driver assistance and robotics<br/>
                </p>

                
            </div>
        </div>
    </div>
</section>



<section id="services">   

<div class="container" id ="program">
        <div class="row-fluid">
            <div class="span9">
                <h1>Program &#38; Accepted Papers</h1>
                <p class="lead">
                The workshop will take place on June 18, 2018.</br>
                Please, check below for the titles and abstracts of the invited talks.
                </p>
		<table class="lead" style="border-style: groove;width:1200px" border="1" >
		<tr>
			<th width="20%">Time</th>
    			<th width="40%">Event</th>
    			<th width="40%">Description</th>
  		</tr>
		<tr><td>09.00 - 09.10</td> <td> Welcome to the workshop </td> <td> Information</td> </tr>
        <tr><td>09.10 - 09.40</td> <td> <a href="http://www0.cs.ucl.ac.uk/staff/I.Kokkinos/"> Iasonas Kokkinos (Facebook, UCL)</a> </td> <td> Invited speaker </td> </tr>
        <tr><td>09.40 - 10.10</td> <td> <a href="https://web.eecs.umich.edu/~honglak/hl_news.html">Honglak Lee (U. Michigan, Google Brain)</a> </td> <td>  Invited speaker </td> </tr>

		<tr><td>10.10 - 10.50</td> <td> Break </td> <td> Coffee </td> </tr>
        
        <tr><td>10.50 - 11.10</td> <td> <a href="">Temporal Reasoning in Videos using Convolutional Gated Recurrent Units</a>, by Debidatta Dwibedi, Pierre Sermanet, Jonathan Tompsonng Convolutional Gated Recurrent Units</td> <td>Oral Presentation</td> </tr>
        <tr><td>11.10 - 11.30</td> <td> <a href="https://arxiv.org/abs/1711.08200">Temporal 3D ConvNets using Temporal Transition Layer</a>, by Ali Diba, Mohsen Fayyaz, Vivek Sharma, A. Hossein Karami, M. Mahdi Arzani, Rahman Yousefzadeh, Luc Van Gool </td> <td>Oral Presentation</td> </tr>
        <tr><td>11.30 - 12.00</td> <td> <a href="http://feichtenhofer.github.io/">Christoph Feichtenhofer (Facebook)</a> </td> <td>  Invited speaker </br> </td> </tr>
        <tr><td>12.00 - 14.30</td> <td> Break </td> <td> Lunch (on your own) </td> </tr>

        <tr><td>14.30 - 15.00</td> <td> <a href="http://www.ceessnoek.info/"> Cees Snoek (University of Amsterdam)</a> </td> <td> Invited speaker </td> </tr>
        <tr><td>15.00 - 15.20</td> <td> <a href="https://arxiv.org/abs/1710.08518">ContextVP: Fully Context-Aware Video Prediction</a>, by Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, Petros Koumoutsakos</a> </td> <td>Oral Presentation</td> </tr>
        <tr><td>15.20 - 16.30</td> <td> Break </td> <td> Afternoon Break &#38; Poster Session </td> </tr>

        <tr><td>16.30 - 17.00</td> <td> <a href="http://www.di.ens.fr/~laptev/"> Ivan Laptev (INRIA)</a> </td> <td> Invited speaker </td> </tr>
        <tr><td>17.00 - 17.15</td> <td> Organizers </td> <td> Closing remarks </td> </tr>

		</table>
                
            </div>
        </div>

        <h3>Invited Talks</h3>
        <p class="lead">
        <b>Invited Speaker:</b> Iasonas Kokkinos (Facebook, UCL) </br>
        <b>Title:</b> To be announced </br>
        <b>Abstract:</b> To be announced </br>

        </br>

        <b>Invited Speaker:</b> Honglak Lee (U. Michigan, Google Brain) </br>
        <b>Title:</b> To be announced </br>
        <b>Abstract:</b> To be announced </br>

        </br>

        <b>Invited Speaker:</b> Cees Snoek (University of Amsterdam) </br>
        <b>Title:</b> To be announced </br>
        <b>Abstract:</b> To be announced </br>

        </br>

        <b>Invited Speaker:</b> Christoph Feichtenhofer (Facebook) </br>
        <b>Title:</b> What have we learned from deep representations for action recognition? </br>
        <b>Abstract:</b> In this talk, I will shed light on deep spatiotemporal representations by visualizing what two-stream models have learned in order to recognize actions in video. I show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions. Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes. Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncracies of training data and to explain failure cases of the system.</br>

        </br>

        <b>Invited Speaker:</b> Ivan Laptev (INRIA) </br>
        <b>Title:</b> To be announced </br>
        <b>Abstract:</b> To be announced </br>

        </p>

        <h3>Accepted papers</h3> 
        <p class="lead">
        - <a href="https://arxiv.org/abs/1711.08200">Temporal 3D ConvNets using Temporal Transition Layer</a> </br> <i>Luc Van Gool, Vivek Sharma, Ali Diba, Rahman Yousefzadeh, Mohammad Mahdi Arzani, Mohsen Fayyaz, Ami Karami </i></br>
        - <a href="https://arxiv.org/abs/1805.04026">Towards an Unequivocal Representation of Actions</a> </br> <i>Dima Damen, Michael Wray, Davide Moltisanti </i></br>
        - <a href="https://arxiv.org/abs/1805.04136">Unsupervised Deep Representations for Learning Audience Facial Behaviors</a> </br> <i>Suman Saha, Rajitha Navarathna, Romann M. Weber, Leonhard Helminger </i></br>
        - <a href="https://arxiv.org/abs/1805.03897">Dealing with sequences in the RGBDT space</a> </br> <i>Gabriel Moya-Alcover, Antoni Jaume-i-Cap√≥, Javier Varona</i> </br>
        - <a href="">Temporal Reasoning in Videos using Convolutional Gated Recurrent Units</a> </br> <i>Debidatta Dwibedi, Jonathan Tompson, Pierre Sermanet </i></br>
        - <a href="https://arxiv.org/abs/1710.08518">ContextVP: Fully Context-Aware Video Prediction</a> </br> <i>Wonmin Byeon, Petros Koumoutsakos, Qin Wang, Rupesh Kumar Srivastava </i></br>
        - <a href="https://arxiv.org/abs/1805.04668">I Have Seen Enough: A Teacher Student Network for Video Classification Using Fewer Frames</a> </br> <i>Shweta Bhardwaj, Mitesh M. Khapra </i></br>
        </p>


    </div>

</section>



<section id="dates">
    <div class="container" >
        <div class="row-fluid">
            <div class="span9">
                <h1>Important Dates</h1>
                <p class="lead">Together with the <a href="http://cvpr2018.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2018.</br>
                Date of the workshop: June 18th, 2018, 09.00-17.15</p>  
              
                <p class="lead">
                <b>Submission Deadline:</b> March 16, 2018 </br>
                <b>Notifications to authors by:</b> April 13, 2018</br>
                <b>Finalized workshop program by:</b> April 20, 2018 </br>
                <b>Papers posted on arXiv (non-archival) by:</b> May 4, 2018 </br>
                </p>
            </div>
        </div>
    </div>
</section>

<section id="services">
    <div class="container" id="submission">
        <div class="row-fluid">
            <div class="span9">

                <h1>Submission</h1>                
                <h3>Constructive discussion</h3>
                <p class="lead">
                The workshop's goal is a constructive, creative and open conversation. In principle we accept all papers with interesting ideas.
                <!-- All reviews will be made publicly available. Reviewers can choose to remain anonymous or to reveal their identity to encourage collaboration and positive feedback. We include poster presentations and will select a few of the best and bravest papers for an oral presentation.  -->
                </p>

		<h3>Instructions</h3>
                <p class="lead">
                     Authors can submit 4 Page papers which will be peer reviewed. However, they will not be include in the proceedings. Please follow the CVPR 2018 camera ready format as per the instructions given <a href="http://cvpr2018.thecvf.com/submission/main_conference/author_guidelines#submission_instructions">here</a> but limit your paper to 4 pages excluding references.
<!-- 		You can submit papers in two different formats. <br><br><strike>
1.Full paper submission should include 8 pages of text and should use the CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a>. Full  paper submission should include 8 pages (excluding references) and will be included in the proceedings of the CVPR17 workshops. Therefore, the deadline for full paper submission is 7th April 2017.<br><br></strike>

2. Authors can also submit 4 Page papers which will be peer reviewed. However, they will not be include in the proceedings. Please follow the 
 CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a> but limit your paper to 4 pages excluding references.
<br><br>

All papers should have the names of the authors, institute and the email address in the header of the paper as per the camera ready format of CVPR 2017. Authors are encouraged to upload their papers in archive. -->
                </p>

        <h3>Program Commitee</h3>
        <p class="lead">
        Jakub Tomczak, Hakan    Bilen, Noureldien   Hussein, Jan    van Gemert, Silvia-Laura    Pintea, Osman   Kayhan, Jack    Valmadre, Amir  Ghodrati, Efstratios    Gavves, Lorenzo Torresani, Amir Ghodrati, Tom   Runia, Christoph    Feichtenhofer, Ross Goroshin, Chen  Huang, Makarand Tapaswi, Joseph Tighe, Du   Tran, Carl  Vondrick, Heng  Wang, Limin Wang
        </p>


<!--                 <h3>Submit</h3>                
                <p class="lead">
                Please use <a href="https://openreview.net/group?id=cv-foundation.org/CVPR/2017/BNMW">OpenReview</a> to submit your paper.
                TBD

                </p>

                <h3>Proceedings</h3>                
                <p class="lead">
                N/A
                </p>

 -->
                
            </div>
        </div>
    </div>
</section>



<section id="registration">
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>Registration & venue</h1>
                <p class="lead">The workshop is together with the <a href="http://cvpr2018.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2018.</p>
                <p class="lead">
                 Accepted papers must have at least one registered author (this can be a student). 
                 </p>


                <p class="lead">
                Venue TBD.</p>
                
            </div>
        </div>
    </div>
</section>



<section id="services">
    <div class="container" id="organizers">
        <div class="row-fluid">
            <div class="span9">
                <h1>Organizers</h1>
                <p class="lead">
                <a href="http://www.cs.dartmouth.edu/~lorenzo/home.html">Lorenzo Torresani</a>,
                <a href="http://www.egavves.com/">Efstratios Gavves</a>,
                <a href="http://users.cecs.anu.edu.au/~basura/">Basura Fernando</a>,
                <a href="http://www.cs.cmu.edu/~rahuls/">Rahul Sukthankar</a>,
                <a href="http://jvgemert.github.io/"> Jan van Gemert</a>,
                <a href="https://www.robots.ox.ac.uk/~az/"> Andrew Zisserman.</a>
                </p>

                <h1>Advisory Committee</h1>
                <p class="lead">
                <a href="http://www.ceessnoek.info/">Cees G.M. Snoek</a>,
                <a href="https://sites.google.com/site/balamanohar/">Manohar Paluri</a>,
                <a href="https://homes.esat.kuleuven.be/~tuytelaa/">Tinne Tuytelaars</a>,
                <a href="http://homepages.inf.ed.ac.uk/hbilen/">Hakan Bilen</a>.
                </p>
                
            </div>
        </div>
    </div>
</section>



<!--Footer-->
<footer id="footer">
    <div class="container">
        <div class="row-fluid">
            <div class="span5 cp">
                &copy; 2013 <a target="_blank" href="http://shapebootstrap.net/" title="Free Twitter Bootstrap WordPress Themes and HTML templates">ShapeBootstrap</a>. All Rights Reserved.
            </div>
            <!--/Copyright-->

            <div class="span1">
                <a id="gototop" class="gototop pull-right" href="#"><i class="icon-angle-up"></i></a>
            </div>
            <!--/Goto Top-->
        </div>
    </div>
</footer>
<!--/Footer-->

<script src="js/vendor/jquery-1.9.1.min.js"></script>
<script src="js/vendor/bootstrap.min.js"></script>
<script src="js/main.js"></script>
<!-- Required javascript files for Slider -->
<script src="js/jquery.ba-cond.min.js"></script>
<script src="js/jquery.slitslider.js"></script>
<!-- /Required javascript files for Slider -->

<!-- SL Slider -->
<script type="text/javascript"> 
$(function() {
    var Page = (function() {

        var $navArrows = $( '#nav-arrows' ),
        slitslider = $( '#slider' ).slitslider( {
            autoplay : false
        } ),

        init = function() {
            initEvents();
        },
        initEvents = function() {
            $navArrows.children( ':last' ).on( 'click', function() {
                slitslider.next();
                return false;
            });

            $navArrows.children( ':first' ).on( 'click', function() {
                slitslider.previous();
                return false;
            });
        };

        return { init : init };

    })();

    Page.init();
});
</script>
<!-- /SL Slider -->
</body>
</html>
